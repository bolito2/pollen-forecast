{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies and pollen data. 260771 total entries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from datetime import datetime, date, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "f = h5py.File('pollen_scraped.h5', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names of the pollen measuring stations and types of pollen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pollen_names = ['alnus', 'alternaria', 'artemisa', 'betula', 'carex',\n",
    "                'castanea', 'cupresaceas', 'fraxinus', 'gramineas', 'mercurialis',\n",
    "                'morus', 'olea', 'palmaceas', 'pinus', 'plantago', 'platanus',\n",
    "                'populus', 'amarantaceas', 'quercus', 'rumex', 'ulmus', 'urticaceas']\n",
    "\n",
    "pollen_stations = ['albacete', 'alcazar', 'alicante', 'almeria', 'avila', 'badajoz',\n",
    "         'barcelona', 'barcelona-uab', 'bejar', 'bilbao', 'burgos', 'burjassot', 'caceres',\n",
    "         'cadiz', 'cartagena', 'castellon-de-la-plana', 'ciudad-real', 'cordoba', 'coru単a',\n",
    "         'cuenca', 'elche', 'gerona', 'granada', 'gijon', 'guadalajara', 'huelva', 'huesca',\n",
    "         'jaen-hospital', 'jaen', 'jativa', 'las-palmas', 'leon', 'lerida', 'logro単o',\n",
    "          'madrid-subiza', 'madrid-hospital', 'malaga', 'murcia', 'oviedo', 'palencia',\n",
    "          'palma-mallorca', 'pamplona', 'ponferrada', 'pontevedra', 'salamanca', 'san-sebastian',\n",
    "          'santa-cruz-tenerife', 'santander', 'santiago-compostela', 'segovia', 'sevilla-macarena',\n",
    "          'sevilla-tomillar', 'soria', 'talavera', 'tarragona', 'teruel', 'toledo', 'torrelavega',\n",
    "           'tudela', 'valencia', 'valladolid', 'vitoria', 'zamora', 'zaragoza']\n",
    "\n",
    "weather_stations = {'albacete': '8178D', 'alcazar':'4121', 'alicante': '8025', 'almeria': '6325O',\n",
    "                    'avila': '2444', 'badajoz' : '4452', 'barcelona': '0201D', 'barcelona-uab': '0200E',\n",
    "                    'bejar': '2870', 'bilbao': '1082', 'burgos':'2331', 'burjassot':'8414A', \n",
    "                    'caceres': '3469A', 'cadiz': '5973', 'cartagena': '7012C', 'castellon-de-la-plana':'8500A',\n",
    "                   'ciudad-real': '4121', 'cordoba': '5402', 'coru単a': '1387', 'cuenca': '8096',\n",
    "                   'elche': '8019', 'gerona': '0367', 'granada':'5530E', 'gijon': '1208H', 'guadalajara': '3168C',\n",
    "                   'huelva': '4642E', 'huesca': '9898', 'jaen-hospital': '5270B', 'jaen': '5270B',\n",
    "                   'jativa': '8293X', 'las-palmas': 'C029O', 'leon': '2661', 'lerida': '9771C', 'logro単o':'9170',\n",
    "                   'madrid-subiza': '3196', 'madrid-hospital': '3194U', 'malaga': '6155A', 'murcia': '7012C',\n",
    "                   'oviedo': '1249I', 'palencia': '2400E', 'palma-mallorca': 'B278', 'pamplona': '9262',\n",
    "                   'ponferrada': '1549', 'pontevedra': '1484C', 'salamanca': '2870', 'san-sebastian': '1024E',\n",
    "                   'santa-cruz-tenerife': 'C449C', 'santander': '1111', 'santiago-compostela': '1475X', \n",
    "                   'segovia': '2465', 'sevilla-macarena': '5783', 'sevilla-tomillar': '5783', 'soria':'2030',\n",
    "                   'talavera': '3365A', 'tarragona': '0016A', 'teruel': '8368U', 'toledo': '3260B', \n",
    "                   'torrelavega': '1109', 'tudela': '9434', 'valencia': '8416', 'valladolid': '2422',\n",
    "                   'vitoria': '9091O', 'zamora': '2614', 'zaragoza': '9434'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write all the data processing steps in functions to apply them to each pollen station later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the pollen data(only cupresaceae at the moment) and eliminate outliers(values > 1000 or < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pollen_data(station):\n",
    "    pollen_data = np.array(f[station][:, [0,7]])\n",
    "    pollen_data[:, 1] = np.maximum(np.minimum(pollen_data[:, 1], 1000), 0)\n",
    "    \n",
    "    print('pollen data of', station, 'is of shape', pollen_data.shape)\n",
    "    \n",
    "    return pollen_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column of the pollen data is the date of test, in format YYYYMMDD type=int32, so we will use this method to convert it to python's date format(datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_to_date(integer):\n",
    "    day = integer % 100\n",
    "    integer = int((integer - day)/100)\n",
    "    month = integer % 100\n",
    "    year = int((integer - month)/100)\n",
    "    \n",
    "    return date(year, month, day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes the start and end date of the pollen data, and prints it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_range(pollen_data):\n",
    "    start_date = integer_to_date(pollen_data[0, 0])\n",
    "    end_date = integer_to_date(pollen_data[-1, 0])\n",
    "\n",
    "    print('Tenemos datos desde el {} hasta el {}'.format(start_date, end_date))\n",
    "    \n",
    "    return start_date, end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REST API call to get weather data from AEMET. So easy compared to the pollen data lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAEMETData(url):\n",
    "    response = requests.request(\"GET\", url)\n",
    "    \n",
    "    try:\n",
    "        return response.json()\n",
    "    except ValueError as e:\n",
    "        print('429 recogiendo datos ._.')\n",
    "        \n",
    "        time.sleep(5)\n",
    "        return readAEMETData(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAEMETbatch(fechaini, fechafin, estacion):\n",
    "    url = \"https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/datos/fechaini/{}/fechafin/{}/estacion/{}\".format(fechaini, fechafin, estacion)\n",
    "    querystring = {\"api_key\":\"eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJib2xpdG8yaGRAZ21haWwuY29tIiwianRpIjoiMjUyOTY3MDQtZDYzZS00Zjk2LWE3NTktYTY4MzE2NzVmMjE0IiwiaXNzIjoiQUVNRVQiLCJpYXQiOjE2MDE2NTY1NzQsInVzZXJJZCI6IjI1Mjk2NzA0LWQ2M2UtNGY5Ni1hNzU5LWE2ODMxNjc1ZjIxNCIsInJvbGUiOiIifQ.KzMsubyf4Ux1jxAgu5cGKGZ7rUaGYUreYu8AR0isWjM\"}\n",
    "\n",
    "    headers = {\n",
    "        'cache-control': \"no-cache\"\n",
    "        }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "    print('weather_search:', response.json()['estado'])\n",
    "    #print(response.text)\n",
    "    if response.json()['estado'] == 429:\n",
    "        time.sleep(5)\n",
    "        return getAEMETbatch(fechaini, fechafin, estacion)\n",
    "    \n",
    "    return readAEMETData(response.json()['datos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking into account the maximum possible data in one query, 5 years, we have to make multiple requests to the AEMET API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAEMETdata(start_date, end_date, station):\n",
    "\n",
    "    loop_date = start_date - relativedelta(days=1)\n",
    "    weather_data = []\n",
    "\n",
    "    while (end_date - loop_date).days >= 0:\n",
    "\n",
    "        loop_date = loop_date + relativedelta(days=1)\n",
    "        fechaini = datetime.strftime(loop_date, '%Y-%m-%d') + 'T00:00:00UTC'\n",
    "\n",
    "        loop_date = loop_date + relativedelta(years = 5, days=-7)\n",
    "\n",
    "        if (end_date - loop_date).days < 0:\n",
    "            fechafin = datetime.strftime(end_date, '%Y-%m-%d') + 'T23:59:59UTC'\n",
    "        else:\n",
    "            fechafin = datetime.strftime(loop_date, '%Y-%m-%d') + 'T23:59:59UTC'\n",
    "\n",
    "        #print('fechaini', fechaini, 'fechafin', fechafin)\n",
    "        weather_data = weather_data + getAEMETbatch(fechaini, fechafin, weather_stations[station])\n",
    "    \n",
    "    return weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pollen data has multiple holes, so we eliminate them from the weather data too, to make the dates match. The below code takes care of it, i'm quite proud of it uwu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some stations the weather data has holes too!! xddd\n",
    "We return the new pollen_data because numpy delete creates a copy of the original array while this isn't needed for weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_holes(pollen_data, weather_data):\n",
    "    i = 1\n",
    "    pollen_holes = 0\n",
    "    weather_holes = 0\n",
    "    \n",
    "    while i < min(pollen_data.shape[0], len(weather_data)):\n",
    "        pollen_date = integer_to_date(pollen_data[-i, 0]) \n",
    "        weather_date = datetime.strptime(weather_data[-i]['fecha'], '%Y-%m-%d').date()\n",
    "        \n",
    "        date_difference = (pollen_date - weather_date).days\n",
    "        \n",
    "        if date_difference > 0:\n",
    "            pollen_data = np.delete(pollen_data, pollen_data.shape[0] - i, axis=0)\n",
    "            weather_holes += 1\n",
    "        \n",
    "        if date_difference < 0:\n",
    "            del weather_data[-i]\n",
    "            pollen_holes += 1\n",
    "\n",
    "        if date_difference == 0:\n",
    "            i += 1             \n",
    "    \n",
    "    print('there were {} pollen holes and {} weather holes'.format(pollen_holes, weather_holes))\n",
    "    \n",
    "    pollen_data = pollen_data[-min(pollen_data.shape[0], len(weather_data)):]\n",
    "    weather_data = weather_data[-min(pollen_data.shape[0], len(weather_data)):]\n",
    "        \n",
    "    return pollen_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to check that the pollen and weather data have the same size and that their dates match up. We then return the training size, $m$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_integrity(pollen_data, weather_data):\n",
    "    print('m=', len(weather_data))\n",
    "    \n",
    "    assert(pollen_data.shape[0] == len(weather_data))\n",
    "    \n",
    "    m = len(weather_data)\n",
    "    \n",
    "    for i in range(m):\n",
    "        if integer_to_date(pollen_data[-(i + 1), 0]) != datetime.strptime(weather_data[-(i + 1)]['fecha'], '%Y-%m-%d').date():\n",
    "            print('error at {}, pollen_date is {} and weather_date is {}'.format(i , integer_to_date(pollen_data[-(i + 1), 0]), datetime.strptime(weather_data[-(i + 1)]['fecha'], '%Y-%m-%d').date()))\n",
    "        assert(integer_to_date(pollen_data[-(i + 1), 0]) == datetime.strptime(weather_data[-(i + 1)]['fecha'], '%Y-%m-%d').date())\n",
    "\n",
    "    print('YAHOO!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets format the data into a np.array to feed the RNN\n",
    "\n",
    "n is the number of features, in our case:\n",
    "\n",
    "- Pollen level\n",
    "    - Here I used a log kernel, with the final data being $z = log(x + 1)$. The reasoning behind this is that we really want to detect the huge spikes that appear in the data(when you would see increases in symptoms presumably), while not detecting small variations in 'baseline' seasonal data is really not a problem as this isn't our focus. The $+1$ is to keep the data away from divergence territory. This actually results in a decrease in loss and a significant qualitative improvement of the predictions\n",
    "- Max temperature\n",
    "- Mean temperature\n",
    "- Min temprerature\n",
    "- Max pressure\n",
    "- Min pressure\n",
    "- Mean wind speed\n",
    "- Max wind speed\n",
    "- Precipitations\n",
    "- Wind component in each direction(we compute this separatedly from the other params)\n",
    "    - I noticed that the parameter of direction is in TENS of degrees, so we have to multiply it by ten. Oh, and I forgot to change it to radians so it was basically useless xd\n",
    "    \n",
    "Guess what, the weather data also has holes! And in each category separately!\n",
    "To combat this we will compute exponentially weighted means to use when some parameter is not known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 12 parameters\n"
     ]
    }
   ],
   "source": [
    "params = ['pollen', 'dx', 'dy', 'tmax', 'tmed', 'tmin', 'presMax', 'presMin', 'velmedia', 'racha', 'prec', 'altitud']\n",
    "n = len(params)\n",
    "\n",
    "print('we have {} parameters'.format(n))\n",
    "\n",
    "def process_data(pollen_data, weather_data):\n",
    "    \n",
    "    m = len(weather_data)\n",
    "    \n",
    "    proc_data = np.zeros((m, n), dtype=np.float32)\n",
    "\n",
    "    proc_data[:, 0] = np.log(pollen_data[:, 1] + 1)\n",
    "\n",
    "    beta = 0.9\n",
    "    exp_means = np.zeros(n)\n",
    "    holes = np.zeros(n, dtype=np.int32)\n",
    "\n",
    "\n",
    "    for i in range(m):\n",
    "        if 'prec' in weather_data[i]:\n",
    "            #print(weather_data[i]['prec'])\n",
    "            if weather_data[i]['prec'] == 'Ip':\n",
    "                weather_data[i]['prec'] = '0,0'\n",
    "        if 'dir' in weather_data[i]:\n",
    "            angle = float(weather_data[i]['dir'].replace(',', '.'))*math.pi/18\n",
    "\n",
    "            proc_data[i, 1] = math.cos(angle)\n",
    "            proc_data[i, 2] = math.sin(angle)\n",
    "\n",
    "            exp_means[1] = beta*exp_means[1] + (1 - beta)*math.cos(angle)\n",
    "            exp_means[2] = beta*exp_means[1] + (1 - beta)*math.sin(angle)\n",
    "        else:\n",
    "            proc_data[i, 1] = exp_means[1]/(1-beta**i)\n",
    "            proc_data[i, 2] = exp_means[2]/(1-beta**i)\n",
    "\n",
    "            holes[1] += 1\n",
    "            holes[2] += 1\n",
    "\n",
    "        #We start at 3 because we compute wind direction components separately\n",
    "        for j in range(3, n):       \n",
    "            if params[j] in weather_data[i]:\n",
    "                try:\n",
    "                    proc_data[i, j] = float(weather_data[i][params[j]].replace(',', '.'))\n",
    "                    exp_means[j] = beta*exp_means[j] + (1 - beta)*proc_data[i, j]\n",
    "                except:\n",
    "                    print('exception')\n",
    "                    proc_data[i, j] = exp_means[j]/(1-beta**i)\n",
    "                    holes[j] += 1\n",
    "            else:\n",
    "                proc_data[i, j] = exp_means[j]/(1-beta**i)\n",
    "                holes[j] += 1\n",
    "\n",
    "    print('holes in each parameter:', holes)\n",
    "    print('-----------------------------------')\n",
    "    return proc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now we should normalize each parameter. We also store the mean and standard deviation of the pollen distribution to rectify the predictions later. We have to take care to normalize ALL the pollen data in a single distribution, independently of the station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(proc_data):\n",
    "    \n",
    "    n = proc_data['madrid-subiza'].shape[1]\n",
    "    \n",
    "    mean = np.zeros(n)\n",
    "    std = np.zeros(n)\n",
    "    data_count = 0\n",
    "    \n",
    "    for station in proc_data.keys():\n",
    "        local_count = proc_data[station].shape[0]\n",
    "        data_count += local_count\n",
    "        \n",
    "        for j in range(n):\n",
    "            mean[j] += proc_data[station][:, j].mean()*local_count\n",
    "            std[j] += proc_data[station][:, j].std()*local_count**2\n",
    "    \n",
    "    mean /= data_count\n",
    "    std /= data_count**2\n",
    "    \n",
    "    for station in proc_data.keys():\n",
    "        for j in range(n):\n",
    "            proc_data[station][:, j] = (proc_data[station][:, j] - mean[j])/std[j]\n",
    "    return mean[0], std[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the data in windows of window_size, where we will use the first window_size - 1 data points to predict the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(data):\n",
    "    windows = np.zeros((data.shape[0] - window_size + 1, window_size, data.shape[1]))\n",
    "    \n",
    "    for i in range(data.shape[0] - window_size + 1):\n",
    "        windows[i] = data[i:i+window_size]\n",
    "        \n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the algorithm perform well, we should start using it only in pollen season. To do this, we will screen the data in batches to see where the pollen levels hare higher than zero(higher than the mean because we used normalization) and only feed those portions to the window generator. If there are consecutive portions in pollen season we will feed them together so windows can be created between the two(this way we increase the training size a bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(proc_data):   \n",
    "    m, n = proc_data.shape\n",
    "\n",
    "    start_season = -1\n",
    "\n",
    "    XY_total = np.zeros((0, window_size, n))\n",
    "    \n",
    "    for i in range(m // batch_size):\n",
    "        sum_pollen = np.sum(proc_data[i*batch_size:(i + 1)*batch_size,0])\n",
    "        \n",
    "        if sum_pollen > proc_data[:, 0].mean()*batch_size:\n",
    "            if start_season == -1:\n",
    "                start_season = i\n",
    "        else:\n",
    "            if start_season >= 0:\n",
    "                XY_total = np.append(XY_total, sliding_windows(proc_data[start_season*batch_size:i*batch_size]), axis=0)\n",
    "                start_season = -1\n",
    "                \n",
    "    print('XY_total.shape:', XY_total.shape)\n",
    "    return XY_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then shuffle the data and split it between X and Y, train dev and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(XY_total, train_rate, dev_rate):\n",
    "    np.random.shuffle(XY_total)\n",
    "    \n",
    "    train_set = XY_total[:int(XY_total.shape[0]*train_rate), :, :]\n",
    "    dev_set = XY_total[int(XY_total.shape[0]*train_rate):int(XY_total.shape[0]*(train_rate + dev_rate)), :, :]\n",
    "    test_set = XY_total[int(XY_total.shape[0]*(train_rate + dev_rate)):, :, :]\n",
    "    \n",
    "    X_train = train_set[:, :-1]\n",
    "    Y_train = train_set[:, -1, :1]\n",
    "    \n",
    "    X_dev = dev_set[:, :-1]\n",
    "    Y_dev = dev_set[:, -1, :1]\n",
    "\n",
    "    X_test = test_set[:, :-1]\n",
    "    Y_test = test_set[:, -1, :1]\n",
    "    \n",
    "    print('X_train.shape', X_train.shape)\n",
    "    print('X_dev.shape', X_dev.shape)\n",
    "    print('X_test.shape', X_test.shape)\n",
    "    \n",
    "    return X_train, Y_train, X_dev, Y_dev, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$shape(X_{train}) = (m_{train}, size(window) - 1, n)$\n",
    "\n",
    "$shape(Y_{train}) = (m_{train}, n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_total = np.zeros((0, window_size, n))\n",
    "proc_data = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pollen data of albacete is of shape (4508, 2)\n",
      "Tenemos datos desde el 2003-01-01 hasta el 2020-09-20\n",
      "weather_search: 200\n",
      "weather_search: 200\n",
      "weather_search: 200\n",
      "weather_search: 200\n",
      "there were 1965 pollen holes and 4 weather holes\n",
      "m= 4504\n",
      "YAHOO!\n",
      "holes in each parameter: [  0 235 235   0   0   0  55  55 238 235   3   0]\n",
      "-----------------------------------\n",
      "pollen data of alcazar is of shape (2659, 2)\n",
      "Tenemos datos desde el 1997-01-01 hasta el 2017-11-28\n",
      "weather_search: 200\n",
      "weather_search: 200\n"
     ]
    }
   ],
   "source": [
    "start = 'albacete'\n",
    "startIndex = pollen_stations.index(start)\n",
    "\n",
    "for station in pollen_stations[startIndex:]:\n",
    "    pollen_data = get_pollen_data(station) \n",
    "    start_date, end_date = get_date_range(pollen_data)\n",
    "    \n",
    "    weather_data = getAEMETdata(start_date, end_date, station)\n",
    "    \n",
    "    pollen_data = delete_holes(pollen_data, weather_data)\n",
    "    verify_integrity(pollen_data, weather_data)\n",
    "    \n",
    "    proc_data[station] = process_data(pollen_data, weather_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No tenemos buenos datos meteorologicos para guadalajara ni pontevedra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'madrid-subiza'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c4cbe7c5a2d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpollen_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpollen_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean, std: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpollen_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpollen_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mproc_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'proc_data.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mproc_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'proc_data'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-900497f732dc>\u001b[0m in \u001b[0;36mnormalize_data\u001b[0;34m(proc_data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnormalize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'madrid-subiza'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'madrid-subiza'"
     ]
    }
   ],
   "source": [
    "pollen_mean, pollen_std = normalize_data(proc_data)\n",
    "print('mean, std: ',pollen_mean, pollen_std)\n",
    "\n",
    "proc_file = h5py.File('proc_data.h5', 'a')\n",
    "proc_file['proc_data'] = proc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in proc_data.keys():\n",
    "    XY_total = np.append(XY_total, batch_data(proc_data[i]), axis=0)\n",
    "\n",
    "X_train, Y_train, X_dev, Y_dev, X_test, Y_test = split_data(XY_total, 0.85, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
